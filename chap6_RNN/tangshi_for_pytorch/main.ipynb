{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import rnn\n",
    "\n",
    "start_token = 'G'\n",
    "end_token = 'E'\n",
    "batch_size = 64\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_poems1(file_name):\n",
    "    \"\"\"\n",
    "\n",
    "    :param file_name:\n",
    "    :return: poems_vector  have tow dimmention ,first is the poem, the second is the word_index\n",
    "    e.g. [[1,2,3,4,5,6,7,8,9,10],[9,6,3,8,5,2,7,4,1]]\n",
    "\n",
    "    \"\"\"\n",
    "    poems = []\n",
    "    with open(file_name, \"r\", encoding='utf-8', ) as f:\n",
    "        for line in f.readlines():\n",
    "            try:\n",
    "                title, content = line.strip().split(':')\n",
    "                # content = content.replace(' ', '').replace('，','').replace('。','')\n",
    "                content = content.replace(' ', '')\n",
    "                if '_' in content or '(' in content or '（' in content or '《' in content or '[' in content or \\\n",
    "                                start_token in content or end_token in content:\n",
    "                    continue\n",
    "                if len(content) < 5 or len(content) > 80:\n",
    "                    continue\n",
    "                content = start_token + content + end_token\n",
    "                poems.append(content)\n",
    "            except ValueError as e:\n",
    "                print(\"error\")\n",
    "                pass\n",
    "    # 按诗的字数排序\n",
    "    poems = sorted(poems, key=lambda line: len(line))\n",
    "    # print(poems)\n",
    "    # 统计每个字出现次数\n",
    "    all_words = []\n",
    "    for poem in poems:\n",
    "        all_words += [word for word in poem]\n",
    "    counter = collections.Counter(all_words)  # 统计词和词频。\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: -x[1])  # 排序\n",
    "    words, _ = zip(*count_pairs)\n",
    "    words = words[:len(words)] + (' ',)\n",
    "    word_int_map = dict(zip(words, range(len(words))))\n",
    "    poems_vector = [list(map(word_int_map.get, poem)) for poem in poems]\n",
    "    return poems_vector, word_int_map, words\n",
    "\n",
    "def process_poems2(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name:\n",
    "    :return: poems_vector  have tow dimmention ,first is the poem, the second is the word_index\n",
    "    e.g. [[1,2,3,4,5,6,7,8,9,10],[9,6,3,8,5,2,7,4,1]]\n",
    "\n",
    "    \"\"\"\n",
    "    poems = []\n",
    "    with open(file_name, \"r\", encoding='utf-8', ) as f:\n",
    "        # content = ''\n",
    "        for line in f.readlines():\n",
    "            try:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    content = line.replace(' '' ', '').replace('，','').replace('。','')\n",
    "                    if '_' in content or '(' in content or '（' in content or '《' in content or '[' in content or \\\n",
    "                                    start_token in content or end_token in content:\n",
    "                        continue\n",
    "                    if len(content) < 5 or len(content) > 80:\n",
    "                        continue\n",
    "                    # print(content)\n",
    "                    content = start_token + content + end_token\n",
    "                    poems.append(content)\n",
    "                    # content = ''\n",
    "            except ValueError as e:\n",
    "                # print(\"error\")\n",
    "                pass\n",
    "    # 按诗的字数排序\n",
    "    poems = sorted(poems, key=lambda line: len(line))\n",
    "    # print(poems)\n",
    "    # 统计每个字出现次数\n",
    "    all_words = []\n",
    "    for poem in poems:\n",
    "        all_words += [word for word in poem]\n",
    "    counter = collections.Counter(all_words)  # 统计词和词频。\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: -x[1])  # 排序\n",
    "    words, _ = zip(*count_pairs)\n",
    "    words = words[:len(words)] + (' ',)\n",
    "    word_int_map = dict(zip(words, range(len(words))))\n",
    "    poems_vector = [list(map(word_int_map.get, poem)) for poem in poems]\n",
    "    return poems_vector, word_int_map, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, poems_vec, word_to_int):\n",
    "    n_chunk = len(poems_vec) // batch_size\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(n_chunk):\n",
    "        start_index = i * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "        x_data = poems_vec[start_index:end_index]\n",
    "        y_data = []\n",
    "        for row in x_data:\n",
    "            y  = row[1:]\n",
    "            y.append(row[-1])\n",
    "            y_data.append(y)\n",
    "        \"\"\"\n",
    "        x_data             y_data\n",
    "        [6,2,4,6,9]       [2,4,6,9,9]\n",
    "        [1,4,2,8,5]       [4,2,8,5,5]\n",
    "        \"\"\"\n",
    "        # print(x_data[0])\n",
    "        # print(y_data[0])\n",
    "        # exit(0)\n",
    "        x_batches.append(x_data)\n",
    "        y_batches.append(y_data)\n",
    "    return x_batches, y_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    # 处理数据集\n",
    "    # poems_vector, word_to_int, vocabularies = process_poems2('./tangshi.txt')\n",
    "    poems_vector, word_to_int, vocabularies = process_poems1('./poems.txt')\n",
    "    # 生成batch\n",
    "    print(\"finish  loadding data\")\n",
    "    BATCH_SIZE = 100\n",
    "\n",
    "    torch.manual_seed(5)\n",
    "    word_embedding = rnn.word_embedding( vocab_length= len(word_to_int) + 1 , embedding_dim= 100).to(device)\n",
    "    rnn_model = rnn.RNN_model(\n",
    "        batch_sz = BATCH_SIZE,\n",
    "        vocab_len = len(word_to_int) + 1,\n",
    "        word_embedding = word_embedding,\n",
    "        embedding_dim= 100, \n",
    "        lstm_hidden_dim=128\n",
    "    ).to(device)\n",
    "\n",
    "    # optimizer = optim.Adam(rnn_model.parameters(), lr= 0.001)\n",
    "    optimizer=optim.RMSprop(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "    loss_fun = torch.nn.NLLLoss()\n",
    "    # rnn_model.load_state_dict(torch.load('./poem_generator_rnn'))  # if you have already trained your model you can load it by this line.\n",
    "\n",
    "    for epoch in range(30):\n",
    "        batches_inputs, batches_outputs = generate_batch(BATCH_SIZE, poems_vector, word_to_int)\n",
    "        n_chunk = len(batches_inputs)\n",
    "        for batch in range(n_chunk):\n",
    "            batch_x = batches_inputs[batch]\n",
    "            batch_y = batches_outputs[batch] # (batch , time_step)\n",
    "\n",
    "            loss = 0\n",
    "            for index in range(BATCH_SIZE):\n",
    "                x = np.array(batch_x[index], dtype = np.int64)\n",
    "                y = np.array(batch_y[index], dtype = np.int64)\n",
    "                x = Variable(torch.from_numpy(np.expand_dims(x,axis=1))).to(device)\n",
    "                y = Variable(torch.from_numpy(y )).to(device)\n",
    "                pre = rnn_model(x)\n",
    "                loss += loss_fun(pre , y)\n",
    "                if index == 0:\n",
    "                    _, pre = torch.max(pre, dim=1)\n",
    "                    print('prediction', pre.data.tolist()) # the following three lines can print the output and the prediction\n",
    "                    print('b_y       ', y.data.tolist())   # And you need to take a screenshot and then past it to your homework paper.\n",
    "                    print('*' * 30)\n",
    "            loss  = loss  / BATCH_SIZE\n",
    "            if batch % 10 == 0:\n",
    "                print(\"epoch  \",epoch,'batch number',batch,\"loss is: \", loss.data.tolist())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(rnn_model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % 20 ==0:\n",
    "                torch.save(rnn_model.state_dict(), './poem_generator_rnn')\n",
    "                print(\"finish  save model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 如果不是训练阶段 ，请注销这一行 。 网络训练时间很长。\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m, in \u001b[0;36mrun_training\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_training\u001b[39m():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# 处理数据集\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# poems_vector, word_to_int, vocabularies = process_poems2('./tangshi.txt')\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     poems_vector, word_to_int, vocabularies \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_poems1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./poems.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# 生成batch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinish  loadding data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m, in \u001b[0;36mprocess_poems1\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# content = content.replace(' ', '').replace('，','').replace('。','')\u001b[39;00m\n\u001b[0;32m     15\u001b[0m content \u001b[38;5;241m=\u001b[39m content\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m content \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m content \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m（\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m content \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m《\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m content \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m content \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m---> 17\u001b[0m                 \u001b[43mstart_token\u001b[49m \u001b[38;5;129;01min\u001b[39;00m content \u001b[38;5;129;01mor\u001b[39;00m end_token \u001b[38;5;129;01min\u001b[39;00m content:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(content) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(content) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m80\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'start_token' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_word(predict, vocabs):  # 预测的结果转化成汉字\n",
    "    sample = np.argmax(predict)\n",
    "\n",
    "    if sample >= len(vocabs):\n",
    "        sample = len(vocabs) - 1\n",
    "\n",
    "    return vocabs[sample]\n",
    "\n",
    "\n",
    "def pretty_print_poem(poem):  # 令打印的结果更工整\n",
    "    shige=[]\n",
    "    for w in poem:\n",
    "        if w == start_token or w == end_token:\n",
    "            break\n",
    "        shige.append(w)\n",
    "    poem_sentences = poem.split('。')\n",
    "    for s in poem_sentences:\n",
    "        if s != '' and len(s) > 10:\n",
    "            print(s + '。')\n",
    "\n",
    "\n",
    "def gen_poem(begin_word):\n",
    "    # poems_vector, word_int_map, vocabularies = process_poems2('./tangshi.txt')  #  use the other dataset to train the network\n",
    "    poems_vector, word_int_map, vocabularies = process_poems1('./poems.txt')\n",
    "    word_embedding = rnn.word_embedding(vocab_length=len(word_int_map) + 1, embedding_dim=100).to(device)\n",
    "    rnn_model = rnn.RNN_model(batch_sz=64, vocab_len=len(word_int_map) + 1, word_embedding=word_embedding,\n",
    "                                   embedding_dim=100, lstm_hidden_dim=128).to(device)\n",
    "\n",
    "    rnn_model.load_state_dict(torch.load('./poem_generator_rnn', map_location=device, weights_only=True))\n",
    "\n",
    "    # 指定开始的字\n",
    "\n",
    "    poem = start_token+begin_word\n",
    "    word = begin_word\n",
    "    while word != end_token:\n",
    "        input = np.array([word_int_map[w] for w in poem],dtype= np.int64)\n",
    "        input = Variable(torch.from_numpy(input)).to(device)\n",
    "        output = rnn_model(input, is_test=True)\n",
    "        word = to_word(output.data.tolist()[-1], vocabularies)\n",
    "        poem += word\n",
    "        # print(word)\n",
    "        # print(poem)\n",
    "        if len(poem) > 100:\n",
    "            break\n",
    "    return poem[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inital  linear weight \n",
      "日日朝云处，东南望不成。\n",
      "不闻天下事，不见一生人。\n",
      "日月浮云上，风光月上春。\n",
      "风前不见处，风雨夜相如。\n",
      "日月浮云色，风光夜未成。\n",
      "何如一夜月，应见一年春。\n",
      "inital  linear weight \n",
      "红树生时不复，金天不见出门前。\n",
      "不如天下客，不见此时心。\n",
      "日月长无事，风光一夜风。\n",
      "风霜有一夜，风雨夜相如。\n",
      "不是东堂客，何人更有时。\n",
      "inital  linear weight \n",
      "山中有何日，一夜千山春。\n",
      "风月千门色，山中万岁生。\n",
      "云开山色色，风急夜长清。\n",
      "风月春光晚，风清月上春。\n",
      "风光浮不得，山水夜风霜。\n",
      "为有东堂客，何人更不行。\n",
      "inital  linear weight \n",
      "夜来风雨后，日日上清光。\n",
      "万事无人见，无心不有无。\n",
      "一千千万岁，何处是闲人。\n",
      "树色浮生水，山光夜雨收。\n",
      "春风有一色，风雨夜相如。\n",
      "inital  linear weight \n",
      "湖上风光出，山光日日开。\n",
      "不如天下事，不见此时归。\n",
      "日月长无事，风清万事闲。\n",
      "风光浮不出，风雨夜相飞。\n",
      "日月浮光上，风光带雨中。\n",
      "此时无一事，应见一年春。\n",
      "inital  linear weight \n",
      "海上山中月，春风一日长。\n",
      "一年无处处，一日一时春。\n",
      "风月浮云上，山花月上春。\n",
      "风前无处客，风雨夜相迎。\n",
      "风月几时客，山川一夜风。\n",
      "何当更何事，应见一生人。\n",
      "inital  linear weight \n",
      "月色风霜夜，清风出水清。\n",
      "不如天下事，不见此时难。\n",
      "日月长无事，风光一夜风。\n",
      "风霜有清日，风雨夜相如。\n",
      "风月千门下，山川一夜风。\n",
      "何如一夜月，不见一生人。\n",
      "inital  linear weight \n",
      "君子有秦处，今朝不见春。\n",
      "一千千万岁，何处是闲人。\n",
      "树色浮分水，山光夜夜长。\n",
      "风霜有此日，山色几时春。\n",
      "树色清风下，山云月上春。\n",
      "何如一夜月，应是此时生。\n"
     ]
    }
   ],
   "source": [
    "pretty_print_poem(gen_poem(\"日\"))\n",
    "pretty_print_poem(gen_poem(\"红\"))\n",
    "pretty_print_poem(gen_poem(\"山\"))\n",
    "pretty_print_poem(gen_poem(\"夜\"))\n",
    "pretty_print_poem(gen_poem(\"湖\"))\n",
    "pretty_print_poem(gen_poem(\"海\"))\n",
    "pretty_print_poem(gen_poem(\"月\"))\n",
    "pretty_print_poem(gen_poem(\"君\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training()  # 如果不是训练阶段 ，请注销这一行 。 网络训练时间很长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有行格式均符合要求！\n"
     ]
    }
   ],
   "source": [
    "def check_poem_format(file_name):\n",
    "    \"\"\"\n",
    "    检查文件中每一行是否符合“标题:诗句”的格式，\n",
    "    并输出有问题的行号和原因。\n",
    "    \"\"\"\n",
    "    forbidden_chars = [\"_\", \"(\", \"（\", \"《\", \"[\", \"G\", \"E\"]  # 可根据需要调整\n",
    "\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        line_number = 0\n",
    "        error_found = False\n",
    "        for line in f:\n",
    "            line_number += 1\n",
    "            line = line.strip()\n",
    "            if not line:  # 跳过空行\n",
    "                continue\n",
    "\n",
    "            # 检查是否有且仅有一个冒号\n",
    "            parts = line.split(\":\")\n",
    "            if len(parts) != 2:\n",
    "                print(f\"Line {line_number}: 格式错误，冒号数量不为1 -> {line}\")\n",
    "                error_found = True\n",
    "                continue\n",
    "\n",
    "            title, content = parts\n",
    "            if not title:\n",
    "                print(f\"Line {line_number}: 标题为空 -> {line}\")\n",
    "                error_found = True\n",
    "            if not content:\n",
    "                print(f\"Line {line_number}: 内容为空 -> {line}\")\n",
    "                error_found = True\n",
    "\n",
    "        if not error_found:\n",
    "            print(\"所有行格式均符合要求！\")\n",
    "\n",
    "# 示例调用：\n",
    "check_poem_format(\"poems.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
